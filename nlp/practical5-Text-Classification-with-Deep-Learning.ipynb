{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli/\"> <img src=\"https://developer.download.nvidia.com/training/images/DLI%20Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Text classification is a classic problem in Natural Language Processing.  Given multiple individual spans of texts (sentences, paragraphs, documents, etc.), the task is to assign each span one or multiple labels - or *classes* - out of $k$ possible ones.  Some possible applications of text classification are:\n",
    "\n",
    "* Genre identification - Does this text contain news, sports, finance, etc?  \n",
    "* Language detection - Is this text in English, German or any other language?\n",
    "* Sentiment analysis - What type of sentiment (positive/negative/neutral) is present in this text?  Additionally, if multiple subjects or topics are discussed, what sentiment is associated with each such subject/topic? \n",
    "\n",
    "A particular type of text classification is the problem of *authorship attribution*.  In this task we are given some documents and a set of possible authors.  We then assign each document with a set of the authors who we believe wrote the document.  Presumably, we also have a large set of documents for which we know the authors already so we can extract features and characteristics to help us with the unknown ones.  \n",
    "\n",
    "Authorship attribution is a well-studied problem which led to the field of [Stylometry](https://en.wikipedia.org/wiki/Stylometry).  As with many other NLP problems, it has benefited greatly from the increase in available computer power, data and advanced machine learning techniques.  All of these make authorship attribution a natural candidate for the use of deep learning (DL).  In particular, we can benefit from DL's ability to automatically extract the relevant features for a specific problem.\n",
    "\n",
    "In this lab we will focus on the following:\n",
    "1.  Building a *linguistic style model* to extract author-specific features from a set of texts (known as a *corpus*)\n",
    "2.  Using these features for building a classification model for authorship attribution\n",
    "3.  Applying the model for identifying the author of a set of unknown documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Federalist Papers\n",
    "\n",
    "The [Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers)[1] are a set of essays written between 1787 and 1788 by [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton), [James Madison](https://en.wikipedia.org/wiki/James_Madison) and [John Jay](https://en.wikipedia.org/wiki/John_Jay).  Initially published under the pseudonym 'Publius', their intent was to encourage the ratification of the then-new Constitution of the United States of America.  In later years, a list emerged where the author of each one of the 85 papers was identified.  Nevertheless, for a subset of these papers the author is still in question.  The problem of the Federalist Papers authorship attribution has been a subject of much research in statistical NLP in the past (see the above Wikipedia article for details).   We will try to use Deep Learning to re-create this research.\n",
    "\n",
    "In concrete terms, the problem is identifying - for each one of the disputed papers - whether Alexander Hamilton (AH) or James Madison (JM) are the authors.  We will assume that each paper has a single author (i.e., that no collaboration took place) and that each author has a well-defined writing style that is displayed across all the papers. \n",
    "\n",
    "## Approach\n",
    "We will take the following approach with this problem:\n",
    "* Use the non-disputed documents as labeled data for an end-to-end model.  The model is composed of two distinct parts (see Figure 1 below):\n",
    "\n",
    "    1.  A linguistic  style feature extractor\n",
    "    2.  A classifier\n",
    "    \n",
    "\n",
    "* Use the model to determine the author for each disputed paper\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 1**: Author Attribution Model </center></caption>\n",
    "\n",
    "Let us now examine each model component in turn.\n",
    "\n",
    "### Style Extraction\n",
    "\n",
    "Given a sequence of tokens (subsets of the input text), we would like the extractor to return a representation of this sequence such that sequences with similar stylistic qualities have similar representations.  In other words, we would like to find a mapping from the sequence to a vector space that uses style properties as its basis.  Some possible properties that the extractor may learn include (but are not limited to):\n",
    "* Where, when and how often punctuation is used\n",
    "* Distribution of sentence length\n",
    "* The use of specific vocabulary and syntactic constructions \n",
    "\n",
    "The above features are well-known in stylometry.  We would like, however, for the model to learn both these and other features that may be applicable.\n",
    "\n",
    "In order to use sequences we will need models that can deal with time-varying inputs (i.e., multiple timesteps with different inputs at each timestep).  In our case, we will have one token every timestep and a fixed sequence length.  These will be our *hyperparameters* - a set of parameters that we determine empirically and that do not change during the training.  The model itself will be a Recurrent Neural Net (RNN) and specifically, a variant named [Long Short Term Memory (LSTM)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) that is ideally suited for NLP problems (see Figure 2).\n",
    "\n",
    "<img src=\"images/lstm.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 2**: LSTM Cell </center></caption>\n",
    "\n",
    "Ideally, our feature extractor will be a type of language model.  That is, given a set of tokens the model can predict the next token with high accuracy.  Let's look at an example:\n",
    "\n",
    "> (1)   The cat sat on the X \n",
    "\n",
    "Looking at (1), we can make some assumptions about the token X:\n",
    "* It is likely a noun (or more precisely, a [Noun Phrase](https://en.wikipedia.org/wiki/Noun_phrase)).  Otherwise, (1) may not be a grammatically-correct sentence.\n",
    "* It is likely some piece of furniture.  Otherwise, there would not be agreement with the semantics of the verb *sat*. \n",
    "\n",
    "If we have such a model available - namely, one that predicts sequences that appear in our training data with high probability - the natural conclusion is that is has learned how our authors tend to write and we can then attempt to use that knowledge.  Here we encounter our primary issue:  training language models on words requires potentially millions of examples.  We have a relatively small corpus, so a word language model may severely overfit (in other words, this model may pick up specific patterns that just happen to appear in our corpus).  This is a tough issue - fortunately, in this case it is easy to overcome.  Since our corpus is composed of proper English text (i.e., no foreign characters or emojis) we can simply use *characters* as the tokens and not words.  For example, given the phrase \"to the people of New York\", we will have the sequence:\n",
    "['t', 'o', ' ', 't', 'h', 'e', ' ', 'p',...] \n",
    "Note that whitespace and punctuation are also characters and will be part of the sequences.  This will hopefully assist us in learning the features mentioned above.  \n",
    "\n",
    "Another trick we can use is to use character *embeddings* as opposed to just the characters themselves (for instance, in a 1-hot encoded representation). Recall that embeddings are dense representations of data that can learn the semantics of the domain.  Here the terminology may be a bit confusing, but the final result is that the embedding of a single character can represent features about the *context* of the character: which characters tend to appear before and/or after this one.  This is precisely what we need.\n",
    "\n",
    "So the structure of our style feature encoder is as follows:\n",
    "\n",
    "* A sequence of characters (length of sequence is a hyperparameter)\n",
    "* An embedding layer for characters (dimensionality of the embedding is a hyperparameter)\n",
    "* LSTM layer for learning a sequence representation (dimensionality of LSTM output is a hyperparameter).  We will use the output of the LSTM at the end of the sequence as our output.  This is known as a *context vector* (or *state vector*) - it encodes details about a particular sequence of characters that can be used by the classifier.\n",
    "\n",
    "### Classifier\n",
    "\n",
    "As noted above, the output of the style feature extractor (or *feature encoder*) is a fixed-size vector.  We use this vector as the input to a simple multi-layer feed-forward network.  Each layer in the network can then extract features that help it determine whether the character sequence (that is, the fixed-size *representation*) was written by AH or JM.  The final layer is composed of a single neuron with a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation, so that the entire model's output $y$ is constrained to be $0 \\leq y \\leq 1$.  Hence, the output can be interpreted as the probability that $y = 1$ or equivalently - given that we arbitrarily assigned the value 1 to JM and 0 to AH - that the sequence was written by JM.  This is easily extendable to multiple ($k \\gt 2$) authors by using $k$ neurons in the final layer with a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation.\n",
    "\n",
    "A very useful property of the entire model (both the feature encoder and the classifier) is that it can be trained end-to-end - there is no need for specifically building either the feature encoder or classifier.  Furthermore, it also means that we can use this model to easily infer the author of each of the disputed papers.  For each such document, we perform the following procedure:\n",
    "\n",
    "1.  Break the entire document to sequences of the same length, as determined by the hyperparameter\n",
    "2.  Retrieve an author prediction for each one of these sequences\n",
    "3.  Determine which author has received more 'votes'.  We will then use this author as our prediction for the entire document.  (Note:  in order to have a clear majority, we need to ensure that the number of sequences is odd).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "We begin by setting up the data pre-processing pipeline.  For each one of the authors, we aggregate all the known papers into a single long text.  Recall that we assume that style does not change across the various papers, hence a single text is equivalent to multiple small ones yet it is much easier to deal with programmatically.\n",
    "\n",
    "For each paper of each author we perform the following steps:\n",
    "* Convert all text into lower-case (ignoring the fact that capitalization may be a stylistic property)\n",
    "* Converting all newlines and multiple whitespaces into single whitespaces\n",
    "* Remove any mention of the authors' names, otherwise we risk [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants for Hamilton/Madison\n",
    "AH = 0\n",
    "JM = 1\n",
    "UNKNOWN = -1\n",
    "\n",
    "\n",
    "def preprocess_text(file_path):\n",
    "    \"\"\" Read and preprocess the text from a specific file.\n",
    "        Preprocessing includes:\n",
    "        * Replace newlines by spaces\n",
    "        * Replace double spaces by single spaces\n",
    "        * Lower-cases the text\n",
    "        * Removes the names of the authors\n",
    "        \n",
    "    # Arguments\n",
    "        file_path: the path to read the file from\n",
    "        \n",
    "    # Returns\n",
    "        The preprocessed file\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        text = ' '.join(lines[1:]).replace(\"\\n\", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "\n",
    "# Concatenate all the papers known to be written by Hamilton/Madisson into a single long text\n",
    "all_hamilton, all_madison = '',''\n",
    "for x in os.listdir('./federalist_papers/AH/'):\n",
    "    all_hamilton += preprocess_text('./federalist_papers/AH/' + x)\n",
    "\n",
    "for x in os.listdir('./federalist_papers/JM/'):\n",
    "    all_madison += preprocess_text('./federalist_papers/JM/' + x)\n",
    "    \n",
    "# Print lengths of the large texts\n",
    "print(\"Hamilton text length: {}\".format(len(all_hamilton)))\n",
    "print(\"Madison text length: {}\".format(len(all_madison)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is much more text available for AH than JM.  We will need to address this issue in order to not bias the model towards AH.\n",
    "\n",
    "The next step is to break the long text for each author into many small sequences.  As described above, we empirically choose a length for the sequence and use it throughout the model's lifecycle.  We get our full dataset by labeling each sequence with its author.\n",
    "\n",
    "To break the long texts into smaller sequences we use the *Tokenizer* class from the Keras framework.  In particular, note that we set it up to tokenize according to *characters* and not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# Hyperparameter - sequence length to use for the model\n",
    "SEQ_LEN = 30\n",
    "\n",
    "\n",
    "def make_subsequences(long_sequence, label, sequence_length=SEQ_LEN):\n",
    "    \"\"\" Breaks a large sequence into multiple smaller sequences of specified length\n",
    "    \n",
    "    # Arguments\n",
    "        long_sequence: the long sequence to break into smaller sequences\n",
    "        label: the label to assign to each subsequence\n",
    "        sequence_length: the length of each subsequence\n",
    "        \n",
    "    # Returns\n",
    "        X: matrix of size [len_sequences - sequence_length, sequence_length] with subsequence data\n",
    "        y: matrix of size [len_sequences - sequence_length, 1] with label data\n",
    "    \n",
    "    \"\"\"\n",
    "    len_sequences = len(long_sequence)\n",
    "    X = np.zeros(((len_sequences - sequence_length)+1, sequence_length))\n",
    "    y = np.zeros((X.shape[0], 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i] = long_sequence[i:i+sequence_length]\n",
    "        y[i] = label\n",
    "    return X,y\n",
    "        \n",
    "# We use the Tokenizer class from Keras to convert the long texts into a sequence of characters (not words)\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "\n",
    "# Make sure to fit all characters in texts from both authors\n",
    "tokenizer.fit_on_texts(all_hamilton + all_madison)\n",
    "\n",
    "madison_long_sequence = tokenizer.texts_to_sequences([all_madison])[0]\n",
    "hamilton_long_sequence = tokenizer.texts_to_sequences([all_hamilton])[0]\n",
    "\n",
    "# Convert the long sequences into sequence and label pairs\n",
    "X_madison, y_madison = make_subsequences(madison_long_sequence, JM)\n",
    "X_hamilton, y_hamilton = make_subsequences(hamilton_long_sequence, AH)\n",
    "\n",
    "# Print sizes of available data\n",
    "print(\"Number of characters: {}\".format(len(tokenizer.word_index)))\n",
    "print('Madison sequences: {}'.format(X_madison.shape))\n",
    "print('Hamilton sequences: {}'.format(X_hamilton.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of raw characters to the number of labeled sequences for each author.  Deep Learning requires many examples of each input.  The following code calculates the number of total and unique words in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique words in the text\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts([all_madison, all_hamilton])\n",
    "\n",
    "print(\"Total word count: \", len((all_madison + ' ' + all_hamilton).split(' ')))\n",
    "print(\"Total number of unique words: \", len(word_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:  Do you think a word or a character embedding model is appropriate here?  Write down your reasoning in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to create our train, validation and test sets.  \n",
    "\n",
    "We begin by addressing the discrepancy  in the amounts of data available for AH vs. JM.  We choose a simple solution here by choosing the same number of sequences for AH as are available for JM and discarding the rest.  Depending on the performance of the model, this may or may not be a good idea in general.\n",
    "\n",
    "We then create the various datasets:\n",
    "* The *training* set is used by the model to learn the weights in the neural network.  The model will iterate over this data many times, until performance is deemed to be acceptable.  A single pass through all the data is known as an *epoch*.  Each training loop works on a subset of the data known as a *mini-batch*.  The number of instances in this mini-batch is known as the *batch size*.\n",
    "* The *validation* set is used at the end of each epoch to assess performance of the model.  We present the model with data it *has not* seen before in order to evaluate its ability to *generalize*.  Had we used the training set instead, the model would have no 'motivation' to learn the internal structure of the data - it would just try to 'memorize' the original data.  We stop the training when the validation set performance begins to drop, as this means that the model now specializes (i.e, overfits) on the training set and is losing its ability to deal with unseen data.\n",
    "* The *test* set is the final measure of performance that we report for the model.  Once again, we feed the model with data that it has not seen before in order to see how well it can generalize.  We do not use the validation set as we have already used it to determine when to stop training, so effectively our model is biased towards good validation performance.  The test set is a brand new set of data that should only be used at the end of the model training procedure.\n",
    "\n",
    "We take 80% of the original data for the training set, and use the remaining 20% for test.  We then split the resulting training set again, and use 90% for actual training and the other 10% for validation.\n",
    "\n",
    "**Exercise**:  Make sure that the data is in the proper shape for use in an RNN.  See [here](#hint1 \"X_train = X_train.reshape(-1,SEQ_LEN,1)\n",
    "X_test = X_test.reshape(-1,SEQ_LEN,1)\") for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take equal amounts of sequences from both authors\n",
    "X = np.vstack((X_madison, X_hamilton[:X_madison.shape[0]]))\n",
    "y = np.vstack((y_madison, y_hamilton[:y_madison.shape[0]]))\n",
    "\n",
    "# Break data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# Data is to be fed into RNN - ensure that the actual data is of size [batch size, sequence length, 1]\n",
    "X_train = ##TODO## : Reshape the data to fit an RNN\n",
    "X_test =  ##TODO## : Reshape the data to fit an RNN\n",
    "\n",
    "# Break train set into train and validation\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, train_size=0.9)\n",
    "\n",
    "# Print the shapes of the train, validation and test sets\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_validate shape: {}\".format(X_validate.shape))\n",
    "print(\"y_validate shape: {}\".format(y_validate.shape))\n",
    "\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct the model graph and perform the training procedure.  Notice how each part of the model we describe above is implemented in Tensorflow code.\n",
    "\n",
    "A single training epoch takes around 8.5 minutes on a K80 GPU.  We have therefore provided pretrained weights for the model at 1, 10 and 20 epochs.  Simply run the code with no changes to use the pretrained weights.  If you'd like to perform the training yourself, change the value of the RUN_TRAINING variable in the second cell below to True.  You can also control the number of training epochs using the NUM_EPOCHS variable.\n",
    "\n",
    "**Exercise**:  Complete the code for the tensorflow layers below.\n",
    "\n",
    "See [hint](#hint2 \"result = tf.nn.embedding_lookup(embeddings, sequences)\") for the embedding layer\n",
    "\n",
    "See [hint](#hint3 \"lstm = tf.contrib.rnn.LSTMCell(lstm_size)\") for the LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_model(sequences, embedding_size=100, lstm_size=256):\n",
    "    \"\"\" Build the Tensorflow graph that implements the neural author attribution model\n",
    "    \n",
    "    # Arguments\n",
    "        sequences: character sequence data of size (batch size, sequence lengths, 1)\n",
    "        embedding_size: size of embedding vector to be generated for each character\n",
    "        lstm_size: size of vector that will be output by the LSTM style model\n",
    "        \n",
    "    # Returns\n",
    "        result: output of the entire model, a value between 0 and 1\n",
    "        lstm_output: the last output of the LSTM for each input sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sequences need to be of type integer\n",
    "    sequences = tf.cast(sequences, tf.int32)\n",
    "    \n",
    "    # Placeholder for determining train or test mode\n",
    "    with tf.variable_scope(\"Options\"):\n",
    "        mode = tf.placeholder(tf.string, (), \"mode\")\n",
    "        is_training = tf.equal(\"train\", mode)\n",
    "    \n",
    "    # Layer for selecting character embeddings \n",
    "    with tf.variable_scope(\"Embeddings\"):\n",
    "        embeddings = tf.get_variable(\"embeddings\", (len(tokenizer.word_index)+1, embedding_size,),\n",
    "                                    initializer=tf.initializers.random_uniform(-0.5,0.5))\n",
    "        result = ##TODO## : Add code for embedding layer here\n",
    "                                 \n",
    "    # Style model implemented using LSTM\n",
    "    with tf.variable_scope(\"StyleModel\"):\n",
    "        \n",
    "        # We use an LSTM in lieu of a generic RNN \n",
    "        lstm = ##TODO## : Add code for LSTM layer here\n",
    "        \n",
    "        result = tf.reshape(result, (-1, SEQ_LEN, embedding_size))\n",
    "        result = tf.unstack(result, SEQ_LEN, 1)   # TF-specific way of feeding data to RNN cell\n",
    "        result, _ = tf.contrib.rnn.static_rnn(lstm, result, dtype=tf.float32)\n",
    "        lstm_value = result[-1]                   # Only take the last result from the LSTM\n",
    "\n",
    "    # Fully-connected classification model on top of the LSTM output\n",
    "                                 \n",
    "    with tf.variable_scope(\"Hidden1\"):\n",
    "        w1 = tf.get_variable(\"w1\", (lstm_size, 128), initializer=tf.initializers.random_uniform(-0.07, 0.07))\n",
    "        b1 = tf.get_variable(\"b1\", 128)\n",
    "        result = tf.nn.relu(tf.matmul(lstm_value, w1) + b1)\n",
    "        \n",
    "        # Dropout should only be applied during training\n",
    "        result = tf.layers.dropout(result, rate=0.4, training=is_training) \n",
    "                                 \n",
    "    with tf.variable_scope(\"Hidden2\"):\n",
    "        w2 = tf.get_variable(\"w2\", (128, 64), initializer=tf.initializers.random_uniform(-0.07, 0.07))\n",
    "        b2 = tf.get_variable(\"b2\", 64)\n",
    "        result = tf.nn.relu(tf.matmul(result, w2) + b2)\n",
    "        \n",
    "        # Dropout should only be applied during training\n",
    "        result = tf.layers.dropout(result, rate=0.3, training=is_training)   \n",
    "                                 \n",
    "    with tf.variable_scope(\"Output\"):\n",
    "        w3 = tf.get_variable(\"w3\", (64, 1), initializer=tf.initializers.random_uniform(-0.07, 0.07))\n",
    "        b3 = tf.get_variable(\"b3\", 1)\n",
    "        result = tf.nn.sigmoid(tf.matmul(result, w3) + b3)\n",
    "        \n",
    "    return result, lstm_value\n",
    " \n",
    "   \n",
    "    \n",
    "       \n",
    "def build_loss(logits, labels):\n",
    "    \"\"\" Build the graph operations for calculating loss\n",
    "    \n",
    "    # Arguments\n",
    "        logits: output value of the model, as a logit\n",
    "        labels: real labels of the data\n",
    "        \n",
    "    # Returns\n",
    "        loss: the loss value\n",
    "        num_correct: number of instances that were correctly classified\n",
    "    \"\"\"\n",
    "    loss = tf.losses.log_loss(labels, logits)\n",
    "    \n",
    "    preds = tf.round(logits)\n",
    "    equality = tf.equal(tf.cast(labels, tf.float32), preds)\n",
    "    num_correct = tf.reduce_sum(tf.cast(equality, tf.float32))\n",
    "    \n",
    "    return loss, num_correct\n",
    "                                 \n",
    "\n",
    "def build_training(loss, learning_rate=0.001):\n",
    "    \"\"\" Build the graph operations for performing training\n",
    "    \n",
    "    # Arguments\n",
    "        loss: loss value for the model\n",
    "        learning_rate: the learning rate to use for the training procedure\n",
    "    \"\"\"\n",
    "    return tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.95).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the code to perform the training loop on our data.  Note the use of Tensorflow iterators for importing different datasets without changes to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "### Hyperparameters\n",
    "BATCH_SIZE = 4096  \n",
    "NUM_EPOCHS = 20     # Change this to shorten training time at the expense of model performance\n",
    "\n",
    "\n",
    "\n",
    "def make_dataset(X,y):\n",
    "    \"\"\" Creates a dataset composed of (data, label) instances, to be used for feeding to training\n",
    "    \n",
    "    # Arguments\n",
    "        X: the data to be used for training\n",
    "        y: the labels to be used for training\n",
    "        \n",
    "    # Returns\n",
    "        ds: a Dataset object to be used for creating iterators\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.zip( \n",
    "                        (tf.data.Dataset.from_tensor_slices(X), tf.data.Dataset.from_tensor_slices(y))\n",
    "                       ).shuffle(len(X), reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def evaluate(dataset):\n",
    "    \"\"\" Perform evaluation of the model given a specific dataset\n",
    "    \n",
    "    # Arguments\n",
    "        dataset: the dataset to be used for the evaluation\n",
    "        \n",
    "    # Returns\n",
    "        mean of the loss value over all batches in this dataset\n",
    "        accuracy score for the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an iterator from the input dataset\n",
    "    sess.run(data_iterator.make_initializer(dataset)) \n",
    "    \n",
    "    \n",
    "    total_inputs = 0;\n",
    "    total_correct = 0;\n",
    "    losses = []\n",
    "    try:\n",
    "        \n",
    "        # Iterate over all batches in the iterator - an exception will be thrown to signal that no more \n",
    "        # data is available.\n",
    "        while True:        \n",
    "            logits_value, loss_value, value_correct  = sess.run([logits, loss, num_correct], \n",
    "                                    feed_dict={\"Options/mode:0\":\"test\"}) # Set dropout to test mode\n",
    "            \n",
    "            total_inputs += logits_value.shape[0]\n",
    "            total_correct += value_correct\n",
    "            losses.append(loss_value)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # This exception is expected.  Simply continue.\n",
    "        pass\n",
    "    \n",
    "    return np.mean(losses), total_correct / total_inputs \n",
    "\n",
    "def train():\n",
    "    \"\"\" Perform a single training epoch of a model\n",
    "        \n",
    "    # Returns\n",
    "        mean of the training loss value over all batches in this dataset\n",
    "        accuracy score for the dataset\n",
    "        duration: time elapsed for performing a single epoch \n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    # Initialize an iterator from the training dataset\n",
    "    sess.run(data_iterator.make_initializer(train_ds))\n",
    "\n",
    "        \n",
    "    losses = []\n",
    "    duration = 0\n",
    "            \n",
    "    accuracy = 0\n",
    "    start = time.time()\n",
    "    try:\n",
    "        \n",
    "        # Iterate over all batches in the iterator - an exception will be thrown to signal that no more \n",
    "        # data is available.\n",
    "        while True:\n",
    "            loss_value, _ = sess.run([loss, training_step], feed_dict={\"Options/mode:0\":\"train\"})\n",
    "            losses.append(loss_value)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # This exception is expected.  Simply continue.\n",
    "        duration = time.time() - start\n",
    "\n",
    "    return np.mean(losses), duration\n",
    "\n",
    "\n",
    "def validate():\n",
    "    \"\"\" Evaluate a validation set on a model\n",
    "    \n",
    "    # Returns\n",
    "        Results of evaluating a validation set on a model\n",
    "    \"\"\"\n",
    "    return evaluate(validate_ds)\n",
    "\n",
    "def test():\n",
    "    \"\"\" Evaluate a test set on a model\n",
    "    \n",
    "    # Returns\n",
    "        Results of evaluating a test set on a model\n",
    "    \"\"\"\n",
    "    return evaluate(test_ds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "g = tf.Graph()  \n",
    "with g.as_default():\n",
    "    with tf.name_scope(\"input\"):\n",
    "\n",
    "            # Build training, validation and test datasets of (data, label) instances\n",
    "            train_ds = make_dataset(X_train, y_train)\n",
    "            validate_ds = make_dataset(X_validate, y_validate)\n",
    "            test_ds = make_dataset(X_test, y_test)\n",
    "\n",
    "            # Build iterator of a specific shape (to be used for multiple datasets)\n",
    "            data_iterator = tf.data.Iterator.from_structure(train_ds.output_types, train_ds.output_shapes)\n",
    "\n",
    "            # A single iterator will return batches of data and labels\n",
    "            next_sequence, next_label = data_iterator.get_next()\n",
    "\n",
    "\n",
    "    # Build a model along with its loss and training operators   \n",
    "    logits, lstm_output = build_model(next_sequence)\n",
    "    loss, num_correct = build_loss(logits, next_label)\n",
    "    training_step = build_training(loss)\n",
    "\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=NUM_EPOCHS)\n",
    "\n",
    "    sess = tf.Session(graph=g)\n",
    "    \n",
    "    \n",
    "    ############# Flag for either training or restoring model from file #################\n",
    "    RUN_TRAINING = False\n",
    "    \n",
    "    \n",
    "    if not RUN_TRAINING:\n",
    "        saver.restore(sess, \"/dli/data/checkpoints/model.ckpt-20\"); # Available checkpoints for 1, 10 and 20 epochs\n",
    "        print(\"Restored model from file!\")\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        sess.run(init_op)\n",
    "\n",
    "        # Run training and validation epochs    \n",
    "        for epoch in range(1, NUM_EPOCHS+1):\n",
    "            mean_train_loss, duration = train()\n",
    "            mean_val_loss, accuracy = validate()\n",
    "            save_path = saver.save(sess, \"/dli/data/checkpoints/model.ckpt\", global_step=epoch)\n",
    "            print(\"Epoch {0} ({1:.3f} seconds), training loss {2:.5f}, validation loss {3:.5f}, validation accuracy {4:.3f}\"\\\n",
    "                  .format(epoch, duration, mean_train_loss, mean_val_loss, accuracy))\n",
    "\n",
    "\n",
    "        mean_test_loss, accuracy = test()\n",
    "        print(\"Test loss: {0:.5f}, test accuracy:  {1:.3f}\".format(mean_test_loss, accuracy))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has finished training, compare the validation and test losses to the training loss.  Notice that the test and validation loss values are similar, but not identical.  This indicates that the validation set is a good approximation for the performance of the test set and the model's ability to generalize to new, unseen data.\n",
    "Notice also that the model's training loss is lower that both validation and test loss.  This may indicate that the model is beginning to overfit by modelling the 'noise' in the training data.  A good rule-of-thumb is to stop the training when the validation loss begins to rise while the training loss continues to drop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Once we have a trained model, we can proceed to evaluate the model's performance.  We will perform this evaluation in two ways:\n",
    "1.  Check the output of the style feature encoder - does it coincide with our intuition of what it should be doing?\n",
    "2.  Compare the output of the classifier with the published research on the disputed papers - how accurate is our model?\n",
    "\n",
    "### Observing the Style Features Encoder\n",
    "As described above, the output of the feature encoder is a fixed-size vector.  The dimension of this vector is determined by the *lstm_size* parameter in the *build_model* function above.  It is interesting to see what insights we can get from looking at this vector and how it applies to the overall model's performance.  \n",
    "\n",
    "In the next cell, we create a new dataset that uses the test set and shuffles it.  Recall that the test set contains data that has not been used during the model's training in any way.  We shuffle it to make sure that we get random instances and in particular, that we do not get sequences that follow each other in the actual text.  We then run a single batch of data through the trained model.  However, rather than look at the final output, we take the output from the LSTM layer (which itself follows a character embedding layer).  We also make sure to include the real labels from this data.  Run this cell multiple times to extract subsequent batches for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    embeddings_ds = test_ds.shuffle(len(X_test))\n",
    "    sess.run(data_iterator.make_initializer(embeddings_ds))\n",
    "    lstm_vectors, real_labels = sess.run([lstm_output, next_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to get a visual idea of the style features vector.  In order to do this, we need to apply a [Dimensionality Reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) technique to transform the high-dimensional vector into a 2 or 3-dimensional vector which we can then visualize.  We do this with a technique called [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) and then plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib import ticker\n",
    "\n",
    "pca = PCA(n_components=2, random_state=12345)\n",
    "transformed_values = pca.fit_transform(lstm_vectors)\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.scatter(transformed_values[:,0], transformed_values[:,1], c=real_labels.ravel(), cmap='autumn', alpha=0.4)\n",
    "colorbar = plt.colorbar();\n",
    "\n",
    "def colorbar_labeler(value, _):\n",
    "    if value == float(AH):\n",
    "        return \"AH\"\n",
    "    elif value == float(JM):\n",
    "        return \"JM\"\n",
    "    return \"\"\n",
    "\n",
    "colorbar.formatter = ticker.FuncFormatter(colorbar_labeler)\n",
    "colorbar.update_ticks()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two interesting observations we can make from this plot:\n",
    "1.  The sequences in the test set seem to cluster together for each author, i.e. they are not randomly placed in the figure.  (If you have time, try reloading the model weights after a single epoch and then re-running the plot.)  This coincides with our hypothesis that each author has a specific style and that the model can learn to identify it.  \n",
    "2.  The two clusters have a certain amount of overlap.  This is not surprising, as the authors use many similar words and phrases.  \n",
    "\n",
    "Note, however, that the red cluster (AH) has a number of points (i.e., sequences) that seem to lie deep in the yellow cluster (JM).  Two possible interpretations for this phenomenon are as follows:\n",
    "1.  Either AH occasionally uses vocabulary and style that closely resembles that of JM, or\n",
    "2.  The labels are not correct.  Keep in mind that the data are just sequences from the text and *not* the entire text itself, so labels refer to whether AH or JM wrote a particular *sequence*.  Hence, this may point to the fact that some papers are actually *collaborations* between AH and JM.  What do you think?\n",
    "\n",
    "Try running this extraction and visualization code multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optional: Using TSNE for Visualization__\n",
    "\n",
    "When applying PCA, we chose to use only the 2 most significant principal components which only account for a small amount of variance in the data.  Since we cannot plot more than 3 components, we need to use a different method if we'd like a more detailed look.  Here we apply a method called [T-distributed Stochastic Neighbor Embedding (T-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
    "\n",
    "Because T-SNE is very computationally intensive (and unfortunately, we do not have a GPU implementation) we take a different approach:  we apply PCA to reduce dimensionality of the original vector, and apply T-SNE on the result.\n",
    "\n",
    "In the following cell, change the NUM_DIMENSIONS parameter to control the amount of variance plotted, and the NUM_ITERATIONS parameter to change the computation time of the algorithm.  Note that this may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "NUM_DIMENSIONS = 30   \n",
    "NUM_ITERATIONS = 500   # No less than 250\n",
    "\n",
    "pca = PCA(n_components=NUM_DIMENSIONS, random_state=12345)\n",
    "pca_values = pca.fit_transform(lstm_vectors)\n",
    "print(\"Explained variance: {}%\".format(sum(pca.explained_variance_ratio_ * 100)))\n",
    "\n",
    "tsne = TSNE(n_iter=NUM_ITERATIONS)\n",
    "transformed_values = tsne.fit_transform(pca_values)\n",
    "\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.scatter(transformed_values[:,0], transformed_values[:,1], c=real_labels.ravel(), cmap='autumn', alpha=0.4)\n",
    "colorbar = plt.colorbar();\n",
    "\n",
    "colorbar.formatter = ticker.FuncFormatter(colorbar_labeler)\n",
    "colorbar.update_ticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Model to the Disputed Papers\n",
    "\n",
    "We will now proceed to our original goal:  run the model on the disputed papers and observe the results.  To do that, we simply repeat the procedure we performed for the training data:  we read the original text, preprocess it to remove capitalization and standardize whitespace and convert to sequences.  We then run all sequences through the model and tally the results - the author with more votes is the 'winner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(os.listdir('./federalist_papers/unknown/')):\n",
    "\n",
    "    disputed_text = preprocess_text('./federalist_papers/unknown/' + x)\n",
    "    disputed_long_sequence = tokenizer.texts_to_sequences([disputed_text])[0]\n",
    "    X_sequences, _ = make_subsequences(disputed_long_sequence, UNKNOWN)\n",
    "    X_sequences = X_sequences.reshape((-1,SEQ_LEN,1))\n",
    "    \n",
    "    with g.as_default():\n",
    "        \n",
    "        votes_for_madison = 0\n",
    "        votes_for_hamilton = 0\n",
    "        \n",
    "        dataset = make_dataset(X_sequences, np.zeros((X_sequences.shape[0], 1)))\n",
    "        sess.run(data_iterator.make_initializer(dataset))\n",
    "        \n",
    "        try:\n",
    "            while True:        \n",
    "                predictions  = sess.run(tf.round(logits), feed_dict={\"Options/mode:0\":\"test\"})\n",
    "                counts = np.unique(predictions, return_counts=True)[1]\n",
    "                votes_for_hamilton += counts[AH]\n",
    "                votes_for_madison += counts[JM]\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        print(\"Paper {} is predicted to have been written by {}, {} to {}\".format(\n",
    "                x.replace('paper_','').replace('.txt',''), \n",
    "                (\"Alexander Hamilton\" if votes_for_hamilton > votes_for_madison else \"James Madison\"),\n",
    "                max(votes_for_hamilton, votes_for_madison), min(votes_for_hamilton, votes_for_madison)))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results from our model to those listed [here](https://en.wikipedia.org/wiki/The_Federalist_Papers#Complete_list).  How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Tensorflow session and free GPU resources\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, we discussed the problem of authorship attribution.  We presented the Federalist Papers debate, and built a Deep Learning model to address it.  Finally, we looked at the model internals to get an intuition for how the it encodes stylometric properties.\n",
    "\n",
    "For the Federalist Papers, we know for a fact that each debated paper was written by either Alexander Hamilton or James Madison.  Therefore we used a model that will output one or the other.  More generally, we could have used a model that returns *confidence levels* for each author - we would then have multiple sigmoid outputs, each giving us the probability of the input text having been written by an author.  If no sigmoid exceeds a certain threshold (for example, 0.5), we can declare that text's author as unknown.\n",
    "\n",
    "Authorship attribution is a type of a *text classification* problem, which are very pervasive.  The models and approach you saw here are very relevant to many types of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment: Categorize words\n",
    "\n",
    "\n",
    "Modify [Sense2Vec.py](../../../../edit/tasks/task2/task/Sense2Vec.py) by filling in all the **##TODO##** in the file to categorize all the words into verbs and nouns.\n",
    "\n",
    "Test your solution with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 Sense2Vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After completing this, go back in the browser page you used to open this notebook and click the ASSESS TASK button. If you have categorized your words correctly then you will pass this assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] The complete text of the Federalist Papers are available from Project Gutenberg [here](http://www.gutenberg.org/ebooks/18).  The data has been split into multiple files for your convenience, and those by John Jay have been removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
