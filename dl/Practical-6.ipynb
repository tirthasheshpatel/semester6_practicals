{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical-6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirthasheshpatel/semester6_practicals/blob/main/dl/Practical-6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyf7ErOnFJeT"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow.keras as keras\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrZRsxXvAkb0"
      },
      "source": [
        "import logging\r\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\r\n",
        "logging.getLogger(\"tensorflow\").addHandler(logging.NullHandler(logging.ERROR))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fUi--a86lQd"
      },
      "source": [
        "data = \"I am Tirth Patel.\\nI am a blasphemous coder.\\nI always upload my code on Google Drive.\\nI don't use GitHub.\\nI think coders who use GitHub are dumb.\\nI am smart.\"\r\n",
        "text = data.split('\\n')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6LcTLeYHSCr"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$*&()*+,-/:;<=>?@[\\]^_{}~')\r\n",
        "tokenizer.fit_on_texts(text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FdyJMkKH2Ng",
        "outputId": "6ad06bbe-4813-4456-c0d2-cb6b56f503b8"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "vocab_size"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jf3BFbJIEUY"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(text)\r\n",
        "l = len(sequences)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpOwRqchInaR",
        "outputId": "63a0fb66-6bbb-4ae8-ed00-a486bba9471b"
      },
      "source": [
        "max_len = max([len(seq) for seq in sequences])\r\n",
        "max_len"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y5kl9sbLg1v"
      },
      "source": [
        "X = []\r\n",
        "y = sequences\r\n",
        "for sq in sequences:\r\n",
        "    X.append(sq[:-1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3heNcPE-HtW"
      },
      "source": [
        "max_len = max([len(seq) for seq in sequences])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aqT1o4m8EiH"
      },
      "source": [
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_len+1)\r\n",
        "y = tf.keras.preprocessing.sequence.pad_sequences(y, maxlen=max_len+1)\r\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEj_xWea8x6u",
        "outputId": "53472524-d656-46dd-855c-26fb683e7d2b"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "                                       tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=50),\r\n",
        "                                       tf.keras.layers.SimpleRNN(50, return_sequences=True),\r\n",
        "                                       tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\r\n",
        "                                   ])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 50)          1250      \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, None, 50)          5050      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 25)          1275      \n",
            "=================================================================\n",
            "Total params: 7,575\n",
            "Trainable params: 7,575\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gCKUCx39dqV",
        "outputId": "8d7f2c25-4c52-49e2-be0e-0c01ddf05597"
      },
      "source": [
        "model.compile(optimizer=\"RMSprop\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\r\n",
        "model.fit(X, y, epochs=200)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.2542 - accuracy: 0.0185\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.1721 - accuracy: 0.1296\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.1123 - accuracy: 0.3519\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0507 - accuracy: 0.3519\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9837 - accuracy: 0.3519\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.9125 - accuracy: 0.3519\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8412 - accuracy: 0.4815\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7732 - accuracy: 0.4815\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.7087 - accuracy: 0.4815\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6472 - accuracy: 0.4815\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5883 - accuracy: 0.4630\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5318 - accuracy: 0.4630\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.4775 - accuracy: 0.4815\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4254 - accuracy: 0.5000\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3752 - accuracy: 0.5185\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3269 - accuracy: 0.5370\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2803 - accuracy: 0.5370\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2354 - accuracy: 0.5370\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1924 - accuracy: 0.5556\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1526 - accuracy: 0.5926\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1198 - accuracy: 0.5370\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0889 - accuracy: 0.5556\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0490 - accuracy: 0.5370\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0128 - accuracy: 0.5741\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.9780 - accuracy: 0.5741\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.9454 - accuracy: 0.5741\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9139 - accuracy: 0.5741\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8834 - accuracy: 0.5741\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8537 - accuracy: 0.5741\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8247 - accuracy: 0.5741\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7967 - accuracy: 0.5926\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7697 - accuracy: 0.5741\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7438 - accuracy: 0.6111\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7188 - accuracy: 0.6296\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6934 - accuracy: 0.6481\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6678 - accuracy: 0.6296\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6417 - accuracy: 0.6852\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.6162 - accuracy: 0.6852\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5912 - accuracy: 0.6852\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5669 - accuracy: 0.6852\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5430 - accuracy: 0.6852\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5197 - accuracy: 0.6852\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4967 - accuracy: 0.6852\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4743 - accuracy: 0.7037\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4521 - accuracy: 0.6852\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.4305 - accuracy: 0.7222\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4090 - accuracy: 0.7037\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3879 - accuracy: 0.7222\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3666 - accuracy: 0.7037\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3456 - accuracy: 0.7963\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3244 - accuracy: 0.7037\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3036 - accuracy: 0.7963\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2828 - accuracy: 0.7407\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2626 - accuracy: 0.7963\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2423 - accuracy: 0.7407\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2226 - accuracy: 0.7963\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2030 - accuracy: 0.7407\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1839 - accuracy: 0.8148\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1648 - accuracy: 0.7593\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1462 - accuracy: 0.8148\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1275 - accuracy: 0.7593\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1094 - accuracy: 0.8148\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0911 - accuracy: 0.7963\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0734 - accuracy: 0.8148\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0554 - accuracy: 0.8148\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0381 - accuracy: 0.8148\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0204 - accuracy: 0.8148\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0035 - accuracy: 0.8148\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9863 - accuracy: 0.8148\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9698 - accuracy: 0.8148\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9531 - accuracy: 0.8148\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9371 - accuracy: 0.8333\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9210 - accuracy: 0.8333\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9054 - accuracy: 0.8519\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8898 - accuracy: 0.8333\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8748 - accuracy: 0.8519\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8597 - accuracy: 0.8519\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8452 - accuracy: 0.8519\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8305 - accuracy: 0.8519\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8164 - accuracy: 0.8519\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8022 - accuracy: 0.8704\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7886 - accuracy: 0.8704\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7749 - accuracy: 0.8704\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7618 - accuracy: 0.8704\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7486 - accuracy: 0.8704\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7359 - accuracy: 0.8889\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7232 - accuracy: 0.8704\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7111 - accuracy: 0.8889\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6989 - accuracy: 0.8889\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6873 - accuracy: 0.8889\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6756 - accuracy: 0.8889\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6644 - accuracy: 0.8889\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6532 - accuracy: 0.8889\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6425 - accuracy: 0.8889\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6317 - accuracy: 0.8889\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6214 - accuracy: 0.8889\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6111 - accuracy: 0.8889\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6012 - accuracy: 0.8889\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5913 - accuracy: 0.8889\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5818 - accuracy: 0.8889\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5723 - accuracy: 0.8889\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5631 - accuracy: 0.8889\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5540 - accuracy: 0.8889\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5452 - accuracy: 0.8889\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5365 - accuracy: 0.8889\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5281 - accuracy: 0.8889\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5197 - accuracy: 0.8889\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5116 - accuracy: 0.8889\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5035 - accuracy: 0.8889\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4958 - accuracy: 0.8889\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4881 - accuracy: 0.8889\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4806 - accuracy: 0.8889\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4732 - accuracy: 0.8889\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4661 - accuracy: 0.8889\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4590 - accuracy: 0.8889\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4521 - accuracy: 0.8889\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4453 - accuracy: 0.8889\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4387 - accuracy: 0.8889\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4322 - accuracy: 0.8889\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4259 - accuracy: 0.8889\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4197 - accuracy: 0.8889\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4137 - accuracy: 0.8889\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4078 - accuracy: 0.8889\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4020 - accuracy: 0.8889\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3964 - accuracy: 0.8889\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3909 - accuracy: 0.8889\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3855 - accuracy: 0.8889\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3803 - accuracy: 0.8889\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3752 - accuracy: 0.8889\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3702 - accuracy: 0.8889\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3654 - accuracy: 0.8889\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3607 - accuracy: 0.8889\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3560 - accuracy: 0.8889\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3516 - accuracy: 0.8889\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3472 - accuracy: 0.8889\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3430 - accuracy: 0.8889\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3389 - accuracy: 0.8889\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3349 - accuracy: 0.8889\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3309 - accuracy: 0.8889\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3272 - accuracy: 0.8889\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3235 - accuracy: 0.8889\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3199 - accuracy: 0.8889\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3164 - accuracy: 0.8889\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3130 - accuracy: 0.8889\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3097 - accuracy: 0.8889\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3065 - accuracy: 0.8889\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3034 - accuracy: 0.8889\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3004 - accuracy: 0.8889\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2975 - accuracy: 0.8889\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2947 - accuracy: 0.8889\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2919 - accuracy: 0.8889\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2892 - accuracy: 0.8889\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2866 - accuracy: 0.8889\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2841 - accuracy: 0.8889\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2817 - accuracy: 0.8889\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2793 - accuracy: 0.8889\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2770 - accuracy: 0.8889\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2747 - accuracy: 0.8889\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2726 - accuracy: 0.8889\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2704 - accuracy: 0.8889\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2684 - accuracy: 0.8889\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2664 - accuracy: 0.8889\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2645 - accuracy: 0.8889\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2626 - accuracy: 0.8889\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2608 - accuracy: 0.8889\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2591 - accuracy: 0.8889\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2574 - accuracy: 0.8889\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2557 - accuracy: 0.8889\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2542 - accuracy: 0.8889\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2526 - accuracy: 0.8889\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2511 - accuracy: 0.8889\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2496 - accuracy: 0.8889\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2483 - accuracy: 0.8889\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2469 - accuracy: 0.8889\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2456 - accuracy: 0.8889\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2443 - accuracy: 0.8889\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2431 - accuracy: 0.8889\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2418 - accuracy: 0.8889\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2407 - accuracy: 0.8889\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2395 - accuracy: 0.8889\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2385 - accuracy: 0.8889\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2374 - accuracy: 0.8889\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2364 - accuracy: 0.8889\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2354 - accuracy: 0.8889\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2345 - accuracy: 0.8889\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2335 - accuracy: 0.8889\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2326 - accuracy: 0.8889\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2317 - accuracy: 0.8889\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2309 - accuracy: 0.8889\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2300 - accuracy: 0.8889\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2293 - accuracy: 0.8889\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2285 - accuracy: 0.8889\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2278 - accuracy: 0.8889\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2270 - accuracy: 0.8889\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2264 - accuracy: 0.8889\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2256 - accuracy: 0.8889\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2250 - accuracy: 0.8889\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2243 - accuracy: 0.8889\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2238 - accuracy: 0.8889\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2231 - accuracy: 0.8889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa2e028ed10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xQx1OUe9-bs"
      },
      "source": [
        "def prob_of_input_sentence(model, tokenizer, sentence):\r\n",
        "    print (\"Input Sentence:\", sentence)\r\n",
        "    encoded=tokenizer.texts_to_sequences([sentence])[0]\r\n",
        "    encoded.insert(0, 0)\r\n",
        "    encoded=np.array(encoded)\r\n",
        "    encoded=np.reshape(encoded, newshape=(1, -1))\r\n",
        "    prob = model.predict(encoded, verbose=0)\r\n",
        "    probability=1\r\n",
        "    for i in range (prob. shape[1]-1):\r\n",
        "        probability = probability * prob[0, i, encoded[0, i+1]]\r\n",
        "    print(\"Probability: \", probability)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prxieX0pET2Z",
        "outputId": "48654f40-b81a-406c-9129-1af40986405d"
      },
      "source": [
        "prob_of_input_sentence(model, tokenizer, \"I am Tirth Patel.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence: I am Tirth Patel.\n",
            "Probability:  1.2854818300437701e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6RARvylEaPD",
        "outputId": "d8618173-13cc-4d11-e1a1-6a05eb5ab407"
      },
      "source": [
        "prob_of_input_sentence(model, tokenizer, \"blasphemous\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence: blasphemous\n",
            "Probability:  0.00012432591756805778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbgIGFkFEtWx",
        "outputId": "75bbb19d-3201-4db7-d915-b8a0906edb5c"
      },
      "source": [
        "prob_of_input_sentence(model, tokenizer, \"Google Drive over GitHub\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence: Google Drive over GitHub\n",
            "Probability:  2.9634810912029133e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KI5gl2yE106",
        "outputId": "3ade841f-b63b-4ba8-cd62-9efd92544c75"
      },
      "source": [
        "prob_of_input_sentence(model, tokenizer, \"I am smart.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence: I am smart.\n",
            "Probability:  0.00037150608505503237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au7QR_2pFHM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6678c5a6-4dd3-4040-d76c-639cb7629db6"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\r\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\r\n",
        "with open(filepath) as f:\r\n",
        "    shakespeare_text = f.read()\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPbl3HhH-hGP"
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\r\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr9zsAqS-i0i",
        "outputId": "4c5de433-12c8-4f71-d1d0-1b507fff71e1"
      },
      "source": [
        "tokenizer.texts_to_sequences(['Vicissitude'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[26, 6, 19, 6, 8, 8, 6, 3, 14, 13, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV4CZYU6-j0D",
        "outputId": "6a4cee3e-074b-4afd-f755-151ff59671bf"
      },
      "source": [
        "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjVxDG14-lMV",
        "outputId": "89cc87e5-e4d1-47a5-9a1b-1b79e9adaa3e"
      },
      "source": [
        "max_id = len(tokenizer.word_index)\r\n",
        "max_id"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OjCem2m-oPZ"
      },
      "source": [
        "dataset_size = tokenizer.document_count"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgvPuA5d-pbB",
        "outputId": "ab9657f3-43c7-46ec-fe20-5d593444326b"
      },
      "source": [
        "dataset_size"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfzRRQNs-quL"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYueyveI-uym"
      },
      "source": [
        "train_size = dataset_size * 90 // 100\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBhUtz_U-x5U"
      },
      "source": [
        "n_steps = 100\r\n",
        "window_length = n_steps + 1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-xY5DSF-zEY"
      },
      "source": [
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtCgyetw-0nS"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ceWB_aG-2Db"
      },
      "source": [
        "batch_size = 32\r\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\r\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8qDJ6nC-3lW"
      },
      "source": [
        "dataset = dataset.map(\r\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o31fRp0C-4uF"
      },
      "source": [
        "dataset = dataset.prefetch(1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyUyaryq-8M-"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n",
        "\r\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI38mT88-9WU"
      },
      "source": [
        "class NvidiaUtilizationCallback(keras.callbacks.Callback):\r\n",
        "    def on_epoch_begin(self, epoch, logs):\r\n",
        "        text = !nvidia-smi\r\n",
        "        text = text[9][60:65] + ' GPU utilization'\r\n",
        "        print(text)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADNT79-8-_hB",
        "outputId": "34bec18d-5e1b-4640-979f-27cc2eaf65c2"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\r\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))    \r\n",
        "])\r\n",
        "\r\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\r\n",
        "steps_per_epoch = train_size // batch_size // n_steps\r\n",
        "\r\n",
        "history = model.fit(dataset, epochs = 40, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback, NvidiaUtilizationCallback()])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "  0%  GPU utilization\n",
            "313/313 [==============================] - 33s 8ms/step - loss: 2.9256\n",
            "Epoch 2/40\n",
            " 43%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 2.1227\n",
            "Epoch 3/40\n",
            " 36%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.8820\n",
            "Epoch 4/40\n",
            " 39%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.7226\n",
            "Epoch 5/40\n",
            " 27%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.5505\n",
            "Epoch 6/40\n",
            " 33%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.3864\n",
            "Epoch 7/40\n",
            " 36%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.3183\n",
            "Epoch 8/40\n",
            " 43%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2648\n",
            "Epoch 9/40\n",
            " 41%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2673\n",
            "Epoch 10/40\n",
            " 38%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2550\n",
            "Epoch 11/40\n",
            " 38%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2639\n",
            "Epoch 12/40\n",
            " 42%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2463\n",
            "Epoch 13/40\n",
            " 40%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2459\n",
            "Epoch 14/40\n",
            " 43%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.3134\n",
            "Epoch 15/40\n",
            " 39%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.3020\n",
            "Epoch 16/40\n",
            " 42%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.3544\n",
            "Epoch 17/40\n",
            " 39%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2955\n",
            "Epoch 18/40\n",
            " 40%  GPU utilization\n",
            "313/313 [==============================] - 3s 7ms/step - loss: 1.2814\n",
            "Epoch 19/40\n",
            " 32%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2951\n",
            "Epoch 20/40\n",
            " 41%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2368\n",
            "Epoch 21/40\n",
            " 38%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2608\n",
            "Epoch 22/40\n",
            " 32%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2355\n",
            "Epoch 23/40\n",
            " 37%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.1885\n",
            "Epoch 24/40\n",
            " 27%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.1866\n",
            "Epoch 25/40\n",
            " 31%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2429\n",
            "Epoch 26/40\n",
            " 31%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2229\n",
            "Epoch 27/40\n",
            " 23%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2318\n",
            "Epoch 28/40\n",
            " 28%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2024\n",
            "Epoch 29/40\n",
            " 43%  GPU utilization\n",
            "313/313 [==============================] - 3s 7ms/step - loss: 1.1403\n",
            "Epoch 30/40\n",
            " 25%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.1960\n",
            "Epoch 31/40\n",
            " 34%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2112\n",
            "Epoch 32/40\n",
            " 38%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2523\n",
            "Epoch 33/40\n",
            " 40%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2378\n",
            "Epoch 34/40\n",
            " 40%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2486\n",
            "Epoch 35/40\n",
            " 38%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2667\n",
            "Epoch 36/40\n",
            " 27%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2656\n",
            "Epoch 37/40\n",
            " 38%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2252\n",
            "Epoch 38/40\n",
            " 39%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2504\n",
            "Epoch 39/40\n",
            " 33%  GPU utilization\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 1.2340\n",
            "Epoch 40/40\n",
            " 44%  GPU utilization\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 1.2376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcTO-FB-_Cbh"
      },
      "source": [
        "def preprocess(texts):\r\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\r\n",
        "    return tf.one_hot(X, max_id)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DEtRDOC_DS2"
      },
      "source": [
        "X_new = preprocess([\"I drink wate\"])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7-V5-5_Eqr"
      },
      "source": [
        "Y_pred = np.argmax(model.predict(X_new), axis=-1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kqO9YQsI_FiF",
        "outputId": "30b4598b-d28f-4844-f3a8-2f29571831a0"
      },
      "source": [
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sU0PErD_IIR"
      },
      "source": [
        "def next_char(text, temperature=1):\r\n",
        "    X_new = preprocess([text])\r\n",
        "    y_proba = model.predict(X_new)[0, -1:, :]\r\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\r\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\r\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DubT04ll_Ja-"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\r\n",
        "    for _ in range(n_chars):\r\n",
        "        text += next_char(text, temperature)\r\n",
        "    return text"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cPICPg8r_L2u",
        "outputId": "35ea153c-623d-4f39-dd9a-5bf50685f505"
      },
      "source": [
        "text_1 = complete_text(\"I\", temperature=0.2)\r\n",
        "text_1"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ing surly his honour of an\\nall this will he hath do'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XaWi7fNnAwXw",
        "outputId": "473a62c3-59e3-4fd9-f010-bf3542856bad"
      },
      "source": [
        "text_2 = complete_text(\"I drink wate\", temperature=1)\r\n",
        "text_2"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I drink watery, in the timed\\nsaur ne thus long an on comead,\\nt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FJrLEFdAA6VB",
        "outputId": "e50c8401-d525-441e-80dd-0f7f03a7f6c4"
      },
      "source": [
        "text_3 = complete_text(\"I was walkin\", temperature=0.5)\r\n",
        "text_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I was walking this pleased strent of the ride in this the peac'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaF6ukFsBKLb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}